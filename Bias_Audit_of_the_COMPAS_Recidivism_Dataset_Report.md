This audit examines racial bias in the COMPAS recidivism risk-score dataset using Python and IBM’s AIF360 fairness toolkit. After filtering the data to include only African-American and Caucasian defendants, I constructed binary labels representing actual two-year recidivism outcomes and model predictions derived from COMPAS risk categories (“Medium” and “High” treated as positive predictions).
Fairness metrics were calculated using AIF360’s ClassificationMetric and BinaryLabelDatasetMetric. Before applying any mitigation, there were clear disparities in error rates between racial groups. The false positive rate (FPR) for African-American defendants was 0.448, almost double the Caucasian FPR of 0.235. This means African-Americans were significantly more likely to be incorrectly labeled as high risk when they did not actually reoffend. False negative rates (FNR) also differed, but the most prominent disparity was in false positives. Confusion matrices reinforce this pattern: African-Americans had 805 false positives, compared with 349 for Caucasians, despite similar sample sizes. These findings are consistent with ProPublica’s original analysis and indicate substantial racial bias.
To address this, I applied AIF360’s Reweighing pre-processing algorithm. This technique assigns different instance weights to privileged and unprivileged groups to reduce bias before training a classifier. After retraining a logistic regression model with these weights, several measures improved. The African-American FPR dropped sharply to 0.188, now lower than the post-mitigation Caucasian FPR of 0.327. The disparate impact ratio improved to 0.79, closer to the commonly accepted fairness threshold of 0.8, indicating movement toward statistical parity.
Although reweighing reduces several disparities, it does not eliminate all trade-offs. Some accuracy shifts occurred, and additional methods—such as Equalized Odds post-processing or adversarial debiasing—could be evaluated for more balanced outcomes. Overall, the results demonstrate that the COMPAS algorithm exhibits measurable racial bias and that algorithmic fairness interventions can meaningfully reduce, though not completely resolve, these inequities.sss